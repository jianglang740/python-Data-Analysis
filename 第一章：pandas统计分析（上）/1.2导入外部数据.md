# 数据分析首先就要有数据，本节主要记录了如何导入不同类型的数据到pandas
---
---
---
---
# 如何导入 Excel 数据到 Pandas

## 一、安装必要的库
要使用 Pandas 处理 Excel 数据，首先需要安装以下两个库：
- **pandas**：核心数据处理库
- **openpyxl**：用于读取现代 .xlsx 格式文件

使用 pip 进行安装：
```bash
pip install pandas openpyxl
```

## 二、导入所需库
在 Python 脚本或 Jupyter Notebook 中，导入 Pandas 库：
```python
import pandas as pd
```

## 三、基础导入操作
### 3.1 导入完整 Excel 文件
使用 `pd.read_excel()` 函数读取整个 Excel 文件：
```python
df = pd.read_excel('data.xlsx')
print(df.head())  # 查看数据前几行
```

### 3.2 指定工作表
如果 Excel 文件包含多个工作表，可以通过 `sheet_name` 参数指定：
```python
# 通过表名读取
df = pd.read_excel('data.xlsx', sheet_name='Sheet1')

# 通过索引读取（索引从0开始）
df = pd.read_excel('data.xlsx', sheet_name=0)

# 读取多个工作表
df_dict = pd.read_excel('data.xlsx', sheet_name=['Sheet1', 'Sheet2'])
```

## 四、导入部分数据
### 4.1 读取特定行数
使用 `nrows` 参数读取前几行数据：
```python
# 读取前100行
df = pd.read_excel('data.xlsx', nrows=100)
```

### 4.2 跳过特定行数
使用 `skiprows` 参数跳过开头的行：
```python
# 跳过前5行
df = pd.read_excel('data.xlsx', skiprows=5)

# 指定行号列表（行号从0开始）
df = pd.read_excel('data.xlsx', skiprows=[0, 2, 5])

# 使用函数动态跳过行（例如跳过注释行）
df = pd.read_excel('data.xlsx', skiprows=lambda x: x % 2 == 0)  # 跳过偶数行
```

### 4.3 读取特定列
使用 `usecols` 参数指定读取的列：
```python
# 通过列名读取
df = pd.read_excel('data.xlsx', usecols=['Name', 'Age', 'City'])

# 通过列索引读取（A=0, B=1, ...）
df = pd.read_excel('data.xlsx', usecols=[0, 2, 4])

# 使用范围读取（例如读取A到C列）
df = pd.read_excel('data.xlsx', usecols='A:C')

# 使用函数筛选列（例如只读取以"Sales_"开头的列）
df = pd.read_excel('data.xlsx', usecols=lambda x: x.startswith('Sales_'))
```

### 4.4 读取特定单元格区域
结合 `skiprows` 和 `nrows` 读取特定区域：
```python
# 读取第6行(index=5)开始的20行，第1-3列
df = pd.read_excel('data.xlsx', skiprows=5, nrows=20, usecols=[0, 1, 2])
```

## 五、处理 Excel 表格结构
### 5.1 指定表头行
使用 `header` 参数指定表头所在行：
```python
# 第2行(index=1)作为表头
df = pd.read_excel('data.xlsx', header=1)

# 没有表头，自动生成列名
df = pd.read_excel('data.xlsx', header=None)
```

### 5.2 多级表头处理
对于有合并单元格的多级表头：
```python
# 将第1行和第2行作为多级表头
df = pd.read_excel('data.xlsx', header=[0, 1])

# 访问多级表头数据
print(df.columns)  # 查看列名结构
print(df[('Level1', 'Level2')])  # 访问特定列
```

## 六、数据类型处理
### 6.1 自动数据类型推断
Pandas 会自动推断数据类型，但有时需要手动指定：
```python
# 查看数据类型
print(df.dtypes)

# 强制某列的数据类型
df = pd.read_excel('data.xlsx', dtype={'Age': 'int32', 'Salary': 'float64'})
```

### 6.2 处理日期时间
使用 `parse_dates` 参数解析日期列：
```python
# 将 'Birthdate' 列解析为日期时间类型
df = pd.read_excel('data.xlsx', parse_dates=['Birthdate'])

# 指定日期格式（提高解析速度）
df = pd.read_excel('data.xlsx', parse_dates=['Birthdate'], date_format='%Y-%m-%d')
```

### 6.3 处理缺失值
使用 `na_values` 参数指定额外的缺失值标识：
```python
# 将 'nan', 'nan', '?' 视为缺失值
df = pd.read_excel('data.xlsx', na_values=['nan', 'nan', '?'])
```

## 七、高级导入选项
### 7.1 读取大文件
对于大型 Excel 文件，使用 `chunksize` 参数分块读取：
```python
# 每次读取1000行
chunk_size = 1000
for chunk in pd.read_excel('data.xlsx', chunksize=chunk_size):
    # 处理每个数据块
    print(chunk.shape)
```

### 7.2 导入时过滤数据
使用 `converters` 参数在导入时处理数据：
```python
# 将性别列中的 'M/F' 转换为 'Male/Female'
def convert_gender(x):
    return 'Male' if x == 'M' else 'Female'

df = pd.read_excel('data.xlsx', converters={'Gender': convert_gender})
```

### 7.3 导入时跳过空行
使用 `skip_blank_lines=True` 参数跳过全空行：
```python
df = pd.read_excel('data.xlsx', skip_blank_lines=True)
```

## 八、导入后的基本操作
### 8.1 查看数据基本信息
```python
print('数据基本信息：')
df.info()

# 查看数据集行数和列数
rows, columns = df.shape

if rows < 5:
    # 短表数据（行数少于5）查看全量数据信息
    print('数据全部内容信息：')
    print(df.to_csv(sep='\t', na_rep='nan'))
else:
    # 长表数据查看数据前几行信息
    print('数据前几行内容信息：')
    print(df.head().to_csv(sep='\t', na_rep='nan'))
```

### 8.2 数据清洗与预处理
```python
# 删除包含缺失值的行
df = df.dropna()

# 填充缺失值
df = df.fillna({'Age': 0, 'City': 'Unknown'})

# 重命名列
df = df.rename(columns={'Name': 'FullName', 'Age': 'Years'})
```

## 九、常见问题与解决方案
### 9.1 文件路径问题
- **相对路径**：确保 Excel 文件与 Python 脚本在同一目录，或使用正确的相对路径
- **绝对路径**：使用原始字符串（在路径前加 `r`）避免转义问题
  ```python
  df = pd.read_excel(r'C:\Users\Documents\data.xlsx')
  ```

### 9.2 权限问题
- 确保 Excel 文件未被其他程序打开
- 确保有读取该文件的权限

### 9.3 格式不兼容问题
- .xls 格式文件需安装 `xlrd` 库：`pip install xlrd`
- 加密或受保护的 Excel 文件无法直接读取

### 9.4 性能问题
- 大型 Excel 文件建议转换为 CSV 或 Parquet 格式后再处理
- 使用 `chunksize` 参数分块处理大文件

## 十、完整示例代码
以下是一个完整的示例，展示如何导入 Excel 的部分数据并进行简单处理：

```python
import pandas as pd

# 导入部分数据：跳过前2行，读取前100行，选择A、C、E列
df = pd.read_excel(
    'sales_data.xlsx',
    skiprows=2,
    nrows=100,
    usecols=[0, 2, 4],
    header=None,
    names=['Date', 'Product', 'Amount']
)

# 处理日期列
df['Date'] = pd.to_datetime(df['Date'])

# 填充缺失值
df['Amount'] = df['Amount'].fillna(0)

# 查看数据信息
print('数据基本信息：')
df.info()

# 查看数据集行数和列数
rows, columns = df.shape

if rows < 5:
    # 短表数据（行数少于5）查看全量数据信息
    print('数据全部内容信息：')
    print(df.to_csv(sep='\t', na_rep='nan'))
else:
    # 长表数据查看数据前几行信息
    print('数据前几行内容信息：')
    print(df.head().to_csv(sep='\t', na_rep='nan'))
```
---
---
---



# 如何导入 CSV 数据到 Pandas

## 一、安装必要的库
要使用 Pandas 处理 CSV 数据，首先需要安装 pandas 库：
```bash
pip install pandas
```

## 二、导入所需库
在 Python 脚本或 Jupyter Notebook 中，导入 Pandas 库：
```python
import pandas as pd
```

## 三、基础导入操作
### 3.1 导入完整 CSV 文件
使用 `pd.read_csv()` 函数读取整个 CSV 文件：
```python
df = pd.read_csv('data.csv')
print(df.head())  # 查看数据前几行
```

### 3.2 指定分隔符
如果 CSV 文件使用逗号以外的分隔符（如制表符、分号等），可以通过 `sep` 参数指定：
```python
# 使用制表符分隔的 CSV 文件
df = pd.read_csv('data.tsv', sep='\t')

# 使用分号分隔的 CSV 文件
df = pd.read_csv('data.csv', sep=';')
```

## 四、导入部分数据
### 4.1 读取特定行数
使用 `nrows` 参数读取前几行数据：
```python
# 读取前100行
df = pd.read_csv('data.csv', nrows=100)
```

### 4.2 跳过特定行数
使用 `skiprows` 参数跳过开头的行：
```python
# 跳过前5行
df = pd.read_csv('data.csv', skiprows=5)

# 指定行号列表（行号从0开始）
df = pd.read_csv('data.csv', skiprows=[0, 2, 5])

# 使用函数动态跳过行
df = pd.read_csv('data.csv', skiprows=lambda x: x % 2 == 0)  # 跳过偶数行
```

### 4.3 读取特定列
使用 `usecols` 参数指定读取的列：
```python
# 通过列名读取
df = pd.read_csv('data.csv', usecols=['Name', 'Age', 'City'])

# 通过列索引读取
df = pd.read_csv('data.csv', usecols=[0, 2, 4])

# 使用函数筛选列
df = pd.read_csv('data.csv', usecols=lambda x: x.startswith('Sales_'))
```

### 4.4 读取特定单元格区域
结合 `skiprows` 和 `nrows` 读取特定区域：
```python
# 读取第6行(index=5)开始的20行，第1-3列
df = pd.read_csv('data.csv', skiprows=5, nrows=20, usecols=[0, 1, 2])
```

## 五、处理 CSV 表格结构
### 5.1 指定表头行
使用 `header` 参数指定表头所在行：
```python
# 第2行(index=1)作为表头
df = pd.read_csv('data.csv', header=1)

# 没有表头，自动生成列名
df = pd.read_csv('data.csv', header=None)

# 自定义列名
df = pd.read_csv('data.csv', header=None, names=['col1', 'col2', 'col3'])
```

### 5.2 多级表头处理
对于有合并单元格的多级表头：
```python
# 将第1行和第2行作为多级表头
df = pd.read_csv('data.csv', header=[0, 1])

# 访问多级表头数据
print(df.columns)  # 查看列名结构
print(df[('Level1', 'Level2')])  # 访问特定列
```

## 六、数据类型处理
### 6.1 自动数据类型推断
Pandas 会自动推断数据类型，但有时需要手动指定：
```python
# 查看数据类型
print(df.dtypes)

# 强制某列的数据类型
df = pd.read_csv('data.csv', dtype={'Age': 'int32', 'Salary': 'float64'})
```

### 6.2 处理日期时间
使用 `parse_dates` 参数解析日期列：
```python
# 将 'Birthdate' 列解析为日期时间类型
df = pd.read_csv('data.csv', parse_dates=['Birthdate'])

# 指定日期格式（提高解析速度）
df = pd.read_csv('data.csv', parse_dates=['Birthdate'], date_format='%Y-%m-%d')
```

### 6.3 处理缺失值
使用 `na_values` 参数指定额外的缺失值标识：
```python
# 将 'nan', 'nan', '?' 视为缺失值
df = pd.read_csv('data.csv', na_values=['nan', 'nan', '?'])
```

## 七、高级导入选项
### 7.1 读取大文件
对于大型 CSV 文件，使用 `chunksize` 参数分块读取：
```python
# 每次读取1000行
chunk_size = 1000
for chunk in pd.read_csv('data.csv', chunksize=chunk_size):
    # 处理每个数据块
    print(chunk.shape)
```

### 7.2 导入时过滤数据
使用 `converters` 参数在导入时处理数据：
```python
# 将性别列中的 'M/F' 转换为 'Male/Female'
def convert_gender(x):
    return 'Male' if x == 'M' else 'Female'

df = pd.read_csv('data.csv', converters={'Gender': convert_gender})
```

### 7.3 导入时跳过空行
使用 `skip_blank_lines=True` 参数跳过全空行：
```python
df = pd.read_csv('data.csv', skip_blank_lines=True)
```

## 八、导入后的基本操作
### 8.1 查看数据基本信息
```python
print('数据基本信息：')
df.info()

# 查看数据集行数和列数
rows, columns = df.shape

if rows < 5:
    # 短表数据（行数少于5）查看全量数据信息
    print('数据全部内容信息：')
    print(df.to_csv(sep='\t', na_rep='nan'))
else:
    # 长表数据查看数据前几行信息
    print('数据前几行内容信息：')
    print(df.head().to_csv(sep='\t', na_rep='nan'))
```

### 8.2 数据清洗与预处理
```python
# 删除包含缺失值的行
df = df.dropna()

# 填充缺失值
df = df.fillna({'Age': 0, 'City': 'Unknown'})

# 重命名列
df = df.rename(columns={'Name': 'FullName', 'Age': 'Years'})
```

## 九、常见问题与解决方案
### 9.1 文件路径问题
- **相对路径**：确保 CSV 文件与 Python 脚本在同一目录，或使用正确的相对路径
- **绝对路径**：使用原始字符串（在路径前加 `r`）避免转义问题
  ```python
  df = pd.read_csv(r'C:\Users\Documents\data.csv')
  ```

### 9.2 编码问题
使用 `encoding` 参数指定文件编码：
```python
# 常见编码格式
df = pd.read_csv('data.csv', encoding='utf-8')
df = pd.read_csv('data.csv', encoding='gbk')
df = pd.read_csv('data.csv', encoding='latin-1')
```

### 9.3 引号问题
如果 CSV 文件中包含引号，可以使用 `quoting` 参数处理：
```python
from csv import QUOTE_NONE

# 不处理引号
df = pd.read_csv('data.csv', quoting=QUOTE_NONE)
```

### 9.4 性能问题
- 大型 CSV 文件建议转换为 Parquet 或 Feather 格式后再处理
- 使用 `chunksize` 参数分块处理大文件

## 十、完整示例代码
以下是一个完整的示例，展示如何导入 CSV 的部分数据并进行简单处理：

```python
import pandas as pd

# 导入部分数据：跳过前2行，读取前100行，选择A、C、E列
df = pd.read_csv(
    'sales_data.csv',
    skiprows=2,
    nrows=100,
    usecols=[0, 2, 4],
    header=None,
    names=['Date', 'Product', 'Amount']
)

# 处理日期列
df['Date'] = pd.to_datetime(df['Date'])

# 填充缺失值
df['Amount'] = df['Amount'].fillna(0)

# 查看数据信息
print('数据基本信息：')
df.info()

# 查看数据集行数和列数
rows, columns = df.shape

if rows < 5:
    # 短表数据（行数少于5）查看全量数据信息
    print('数据全部内容信息：')
    print(df.to_csv(sep='\t', na_rep='nan'))
else:
    # 长表数据查看数据前几行信息
    print('数据前几行内容信息：')
    print(df.head().to_csv(sep='\t', na_rep='nan'))
```
---
---
---


# 如何导入 HTML 网页表格到 Pandas

## 一、安装必要的库
要使用 Pandas 处理 HTML 表格数据，首先需要安装以下库：
```bash
pip install pandas beautifulsoup4 lxml html5lib
```
- **pandas**：核心数据处理库
- **beautifulsoup4**：HTML 解析库
- **lxml** 或 **html5lib**：HTML 解析器

## 二、导入所需库
在 Python 脚本或 Jupyter Notebook 中，导入 Pandas 库：
```python
import pandas as pd
```

## 三、基础导入操作
### 3.1 从 HTML 字符串导入表格
如果已有 HTML 内容字符串，可以直接使用 `pd.read_html()` 解析：
```python
html_string = """
<table>
  <tr>
    <th>Name</th>
    <th>Age</th>
    <th>City</th>
  </tr>
  <tr>
    <td>Alice</td>
    <td>28</td>
    <td>New York</td>
  </tr>
  <tr>
    <td>Bob</td>
    <td>34</td>
    <td>Los Angeles</td>
  </tr>
</table>
"""

# 解析 HTML 字符串中的所有表格
dfs = pd.read_html(html_string)

# 获取第一个表格（返回的是 DataFrame 列表）
df = dfs[0]
print(df)
```

### 3.2 从 URL 导入表格
直接从网页 URL 读取表格：
```python
url = 'https://example.com/table.html'
dfs = pd.read_html(url)

# 获取第一个表格
df = dfs[0]
print(df.head())
```

### 3.3 从 HTML 文件导入表格
如果 HTML 内容保存在本地文件中：
```python
# 读取 HTML 文件
with open('table.html', 'r') as f:
    html_content = f.read()

# 解析 HTML 内容中的表格
dfs = pd.read_html(html_content)
df = dfs[0]
print(df)
```

## 四、导入特定表格
### 4.1 通过表格索引选择
`pd.read_html()` 返回一个 DataFrame 列表，可以通过索引选择特定表格：
```python
# 获取第二个表格（索引为1）
df = dfs[1]
```

### 4.2 通过表格属性筛选
使用 `attrs` 参数根据 HTML 属性筛选表格：
```python
# 选择具有特定 ID 的表格
dfs = pd.read_html(html_string, attrs={'id': 'table_id'})

# 选择具有特定类名的表格
dfs = pd.read_html(html_string, attrs={'class': 'table_class'})
```

### 4.3 通过匹配文本筛选
使用 `match` 参数选择包含特定文本的表格：
```python
# 选择包含 "Financial Data" 的表格
dfs = pd.read_html(html_string, match='Financial Data')
```

## 五、处理复杂表格
### 5.1 多级表头处理
对于有合并单元格的多级表头：
```python
# 解析多级表头
dfs = pd.read_html(html_string, header=[0, 1])

# 访问多级表头数据
print(df.columns)  # 查看列名结构
print(df[('Level1', 'Level2')])  # 访问特定列
```

### 5.2 处理表格中的缺失值
使用 `na_values` 参数指定额外的缺失值标识：
```python
# 将 'nan', 'nan', '?' 视为缺失值
dfs = pd.read_html(html_string, na_values=['nan', 'nan', '?'])
```

### 5.3 处理不规则表格
使用 `skiprows` 和 `header` 参数处理不规则表格：
```python
# 跳过前两行，第三行作为表头
dfs = pd.read_html(html_string, skiprows=2, header=0)
```

## 六、数据类型处理
### 6.1 自动数据类型推断
Pandas 会自动推断数据类型，但有时需要手动指定：
```python
# 查看数据类型
print(df.dtypes)

# 强制某列的数据类型
df = dfs[0].astype({'Age': 'int32', 'Salary': 'float64'})
```

### 6.2 处理日期时间
使用 `parse_dates` 参数解析日期列：
```python
# 将 'Date' 列解析为日期时间类型
df = dfs[0]
df['Date'] = pd.to_datetime(df['Date'])
```

## 七、高级导入选项
### 7.1 指定解析器
使用 `flavor` 参数指定解析器：
```python
# 使用 lxml 解析器
dfs = pd.read_html(html_string, flavor='lxml')

# 使用 html5lib 解析器
dfs = pd.read_html(html_string, flavor='html5lib')
```

### 7.2 处理表格中的注释
使用 `comment` 参数跳过注释行：
```python
# 跳过以 # 开头的行
dfs = pd.read_html(html_string, comment='#')
```

### 7.3 导入时过滤数据
结合 BeautifulSoup 预处理 HTML 内容：
```python
from bs4 import BeautifulSoup

# 使用 BeautifulSoup 预处理 HTML
soup = BeautifulSoup(html_string, 'html.parser')
tables = soup.find_all('table')

# 选择特定表格进行解析
df = pd.read_html(str(tables[0]))[0]
```

## 八、导入后的基本操作
### 8.1 查看数据基本信息
```python
print('数据基本信息：')
df.info()

# 查看数据集行数和列数
rows, columns = df.shape

if rows < 5:
    # 短表数据（行数少于5）查看全量数据信息
    print('数据全部内容信息：')
    print(df.to_csv(sep='\t', na_rep='nan'))
else:
    # 长表数据查看数据前几行信息
    print('数据前几行内容信息：')
    print(df.head().to_csv(sep='\t', na_rep='nan'))
```

### 8.2 数据清洗与预处理
```python
# 删除包含缺失值的行
df = df.dropna()

# 填充缺失值
df = df.fillna({'Age': 0, 'City': 'Unknown'})

# 重命名列
df = df.rename(columns={'Name': 'FullName', 'Age': 'Years'})
```

## 九、常见问题与解决方案
### 9.1 网络请求问题
- 使用 requests 库获取网页内容：
  ```python
  import requests

  url = 'https://example.com/table.html'
  response = requests.get(url)
  dfs = pd.read_html(response.text)
  ```

### 9.2 解析失败问题
- 尝试不同的解析器：`flavor=['lxml', 'html5lib']`
- 检查 HTML 结构是否符合预期
- 使用 BeautifulSoup 手动解析复杂结构

### 9.3 编码问题
- 指定网页编码：
  ```python
  response = requests.get(url)
  response.encoding = 'utf-8'  # 指定编码
  dfs = pd.read_html(response.text)
  ```

### 9.4 JavaScript 渲染问题
- `pd.read_html()` 只能处理静态 HTML。对于 JavaScript 渲染的内容，可以使用 Selenium：
  ```python
  from selenium import webdriver

  driver = webdriver.Chrome()
  driver.get(url)
  html_content = driver.page_source
  dfs = pd.read_html(html_content)
  driver.quit()
  ```

## 十、完整示例代码
以下是一个完整的示例，展示如何从网页导入表格数据并进行简单处理：

```python
import pandas as pd

# 从 URL 读取表格
url = 'https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)'
dfs = pd.read_html(url)

# 查找包含特定文本的表格
target_df = None
for df in dfs:
    if 'Country/Territory' in df.columns:
        target_df = df
        break

if target_df is not None:
    # 数据清洗
    df = target_df.copy()
    
    # 查看数据信息
    print('数据基本信息：')
    df.info()

    # 查看数据集行数和列数
    rows, columns = df.shape

    if rows < 5:
        # 短表数据（行数少于5）查看全量数据信息
        print('数据全部内容信息：')
        print(df.to_csv(sep='\t', na_rep='nan'))
    else:
        # 长表数据查看数据前几行信息
        print('数据前几行内容信息：')
        print(df.head().to_csv(sep='\t', na_rep='nan'))
else:
    print("未找到符合条件的表格")
```
